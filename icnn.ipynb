{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fd34ee56-00ea-400a-98f7-5b6660b3d243",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import pathlib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data as data_utils\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pypower as pp\n",
    "from pypower.api import runopf, ppoption\n",
    "ppopt = ppoption(VERBOSE=0, OUT_ALL=0, OPT={'OPF_ALG': 560})\n",
    "\n",
    "from pypower import case9, case14, case30, case57, case118, case300\n",
    "cases = {\n",
    "    \"case9\": case9,\n",
    "    \"case14\": case14,\n",
    "    \"case30\": case30,\n",
    "    \"case57\": case57,\n",
    "    \"case118\": case118,\n",
    "    \"case300\": case300,\n",
    "}\n",
    "\n",
    "def run_opf(case, Pd, Qd):\n",
    "\n",
    "    # Get load bus indices\n",
    "    inds = np.arange(case[\"bus\"].shape[0])\n",
    "    load_buses_idx = list(inds[(case[\"bus\"][:,2] != 0) | (case[\"bus\"][:,3] != 0)].astype(object))\n",
    "    \n",
    "    # Update case load vector\n",
    "    case[\"bus\"][load_buses_idx, 2] = Pd\n",
    "    case[\"bus\"][load_buses_idx,3] = Qd\n",
    "    \n",
    "    # Solve case\n",
    "    results = runopf(case, ppopt)\n",
    "    if results[\"success\"]:\n",
    "        cost = results[\"f\"]\n",
    "    else:\n",
    "        cost = np.nan\n",
    "\n",
    "    return cost\n",
    "\n",
    "# -----------------------------\n",
    "# Standard Feedforward NN Model\n",
    "# -----------------------------\n",
    "class FCNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(FCNN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# -----------------------------\n",
    "# Input Convex Neural Network\n",
    "# -----------------------------\n",
    "class ICNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ICNN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # First hidden layer\n",
    "        self.first_hidden_layer = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # ICNN hidden layers with non-negative W_z\n",
    "        self.second_layer_linear_prim = nn.Linear(512, 512)\n",
    "        self.second_layer_linear_skip = nn.Linear(input_dim, 512)\n",
    "        self.second_layer_act = nn.ReLU()\n",
    "\n",
    "        self.third_layer_linear_prim = nn.Linear(512, 256)\n",
    "        self.third_layer_linear_skip = nn.Linear(input_dim, 256)\n",
    "        self.third_layer_act = nn.ReLU()\n",
    "\n",
    "        self.fourth_layer_linear_prim = nn.Linear(256, 64)\n",
    "        self.fourth_layer_linear_skip = nn.Linear(input_dim, 64)\n",
    "        self.fourth_layer_act = nn.ReLU()\n",
    "\n",
    "        self.fifth_layer_linear_prim = nn.Linear(64, 16)\n",
    "        self.fifth_layer_linear_skip = nn.Linear(input_dim, 16)\n",
    "        self.fifth_layer_act = nn.ReLU()\n",
    "\n",
    "        self.output_layer_linear_prim = nn.Linear(16, 1)\n",
    "        self.output_layer_linear_skip = nn.Linear(input_dim, 1)\n",
    "\n",
    "        # Non-negative weights\n",
    "        self.nonneg_layers = [layer for name, layer in self.named_modules() if\n",
    "                              isinstance(layer, nn.Linear) and \"prim\" in name]\n",
    "\n",
    "        # Enforce non-negative weights at initialization\n",
    "        self.clamp_nonneg_weights()\n",
    "\n",
    "    def clamp_nonneg_weights(self):\n",
    "        for name, layer in self.named_modules():\n",
    "            if isinstance(layer, nn.Linear) and \"prim\" in name:\n",
    "                layer.weight.data.clamp_(min=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        z1 = self.first_hidden_layer(x)\n",
    "        z2 = self.second_layer_act(self.second_layer_linear_prim(z1) + self.second_layer_linear_skip(x))\n",
    "        z3 = self.third_layer_act(self.third_layer_linear_prim(z2) + self.third_layer_linear_skip(x))\n",
    "        z4 = self.fourth_layer_act(self.fourth_layer_linear_prim(z3) + self.fourth_layer_linear_skip(x))\n",
    "        z5 = self.fifth_layer_act(self.fifth_layer_linear_prim(z4) + self.fifth_layer_linear_skip(x))\n",
    "        out = self.output_layer_linear_prim(z5) + self.output_layer_linear_skip(x)\n",
    "        return out\n",
    "\n",
    "# -----------------------------\n",
    "# Training Functions\n",
    "# -----------------------------\n",
    "\n",
    "def train(model, dataloader, optimizer, loss_fn, scheduler=None, max_norm=None):\n",
    "    # Prepare model for training\n",
    "    model.train()\n",
    "    loss_arr = []\n",
    "    for X, y in dataloader:\n",
    "        # Get prediction / loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        loss_arr.append(loss.item())\n",
    "        # Do backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Gradient clip (optional)\n",
    "        if max_norm:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        # Optional clamping (for ICNN)\n",
    "        if isinstance(model, ICNN):\n",
    "            model.clamp_nonneg_weights()\n",
    "    return np.mean(loss_arr)\n",
    "\n",
    "def test(model, dataloader, loss_fn):\n",
    "    model.eval()\n",
    "    loss_arr = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            loss_arr.append(loss.item())\n",
    "    return np.mean(loss_arr)\n",
    "\n",
    "def pred(model, dataloader):\n",
    "    model.eval()\n",
    "    pred_arr = []\n",
    "    with torch.no_grad():\n",
    "        for X, _ in dataloader:\n",
    "            pred = model(X)\n",
    "            pred_arr.append(pred.item())\n",
    "    return np.array(pred_arr)\n",
    "\n",
    "def train_loop(model, train_loader, test_loader, loss_fn, optimizer, scheduler=None, epochs=1000, max_norm=None):\n",
    "    # Loop through epochs of training / testing\n",
    "    train_history, test_history = [], []\n",
    "    for epoch in tqdm(range(epochs), desc=\"Training Epochs\"):\n",
    "        train_loss = train(model, train_loader, optimizer, loss_fn, scheduler=scheduler, max_norm=max_norm)\n",
    "        train_history.append(train_loss)\n",
    "        test_loss = test(model, test_loader, loss_fn)\n",
    "        test_history.append(test_loss)\n",
    "        # Scheduler step\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "    print(\"Done!\")\n",
    "    return model, train_history, test_history\n",
    "\n",
    "def create_train_test_datasets(X, y):\n",
    "\n",
    "    # Create train / test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=5)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def create_batches(X, y, batch_size, shuffle=True):\n",
    "    \n",
    "    # Convert to tensors\n",
    "    X, y = torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "    \n",
    "    # Get training / testing data loaders\n",
    "    data = data_utils.TensorDataset(X, y)\n",
    "    data_loader = data_utils.DataLoader(data, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    return data_loader\n",
    "\n",
    "def train_model(model, train_loader, test_loader, optimizer=\"SGD\", n_epochs=5000, learning_rate=0.1, weight_decay=0.0, scheduler_step_size=None, scheduler_gamma=None, max_norm=None):\n",
    "    \n",
    "    # Select device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using {device} device\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Loss function\n",
    "    loss_fn = nn.MSELoss()  # Mean square error\n",
    "    if optimizer == \"SGD\":\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    elif optimizer == \"Adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    # Scheduler\n",
    "    if (scheduler_step_size is not None) or (scheduler_gamma is not None):\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=scheduler_step_size, gamma=scheduler_gamma)\n",
    "    else:\n",
    "        scheduler = None\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Training...\")\n",
    "    model, train_history, test_history = train_loop(model, train_loader, test_loader, loss_fn, optimizer, epochs=n_epochs, scheduler=scheduler, max_norm=max_norm)\n",
    "    \n",
    "    return model, train_history, test_history\n",
    "\n",
    "def save_model(model, save_path):\n",
    "    # Save model weights\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "\n",
    "def load_model(model, save_path):\n",
    "    # Load saved model weights\n",
    "    model.load_state_dict(torch.load(save_path, weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d32e41-333c-4eae-986c-4656a38e06ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Create Training Data\n",
    "# -----------------------------\n",
    "\n",
    "# Generate training data for f(x) = x^2 - alpha x^4 (non-convex)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "def f(x, alpha=0.):\n",
    "    return x ** 2 - alpha * x ** 4\n",
    "\n",
    "n_samples = 5000\n",
    "X = 4 * np.random.randn(n_samples).reshape(-1, 1)\n",
    "y = f(X)\n",
    "    \n",
    "# -----------------------------\n",
    "# Train Both Models\n",
    "# -----------------------------\n",
    "\n",
    "# Create training/testing data sets\n",
    "X_train, y_train, X_test, y_test = create_train_test_datasets(X, y)\n",
    "train_loader1 = create_batches(X_train, y_train, batch_size=200)\n",
    "train_loader2 = create_batches(X_train, y_train, batch_size=50)\n",
    "test_loader = create_batches(X_test, y_test, batch_size=50)\n",
    "\n",
    "# Initialize models\n",
    "nn_model = FCNN(input_dim=1)\n",
    "icnn_model = ICNN(input_dim=1)\n",
    "\n",
    "# Train models\n",
    "\n",
    "# Phase 1: SGD\n",
    "nn_model, nn_train_history_phase1, nn_test_history_phase1 = train_model(\n",
    "    nn_model,\n",
    "    train_loader1,\n",
    "    test_loader,\n",
    "    optimizer=\"SGD\", \n",
    "    n_epochs=500,\n",
    "    learning_rate=0.01,\n",
    "    weight_decay=1e-9,\n",
    "    scheduler_step_size=64,\n",
    "    scheduler_gamma=0.8,\n",
    "    max_norm=3.0\n",
    ")\n",
    "\n",
    "# Phase 2: Adam\n",
    "nn_model, nn_train_history_phase2, nn_test_history_phase2 = train_model(\n",
    "    nn_model,\n",
    "    train_loader2,\n",
    "    test_loader,\n",
    "    optimizer=\"Adam\", \n",
    "    n_epochs=500,\n",
    "    learning_rate=0.0001,\n",
    "    weight_decay=1e-9,\n",
    "    scheduler_step_size=64,\n",
    "    scheduler_gamma=0.8,\n",
    "    max_norm=3.0\n",
    ")\n",
    "nn_train_history = nn_train_history_phase1 + nn_train_history_phase2\n",
    "nn_test_history = nn_test_history_phase1 + nn_test_history_phase2\n",
    "\n",
    "# Phase 1: SGD\n",
    "icnn_model, icnn_train_history_phase1, icnn_test_history_phase1 = train_model(\n",
    "    icnn_model,\n",
    "    train_loader1,\n",
    "    test_loader,\n",
    "    optimizer=\"SGD\",\n",
    "    n_epochs=500,\n",
    "    learning_rate=0.1,\n",
    "    weight_decay=1e-9,\n",
    "    scheduler_step_size=64,\n",
    "    scheduler_gamma=0.8,\n",
    "    max_norm=3.0\n",
    ")\n",
    "\n",
    "# Phase 2: Adam\n",
    "icnn_model, icnn_train_history_phase2, icnn_test_history_phase2 = train_model(\n",
    "    icnn_model,\n",
    "    train_loader2,\n",
    "    test_loader,\n",
    "    optimizer=\"Adam\",\n",
    "    n_epochs=500,\n",
    "    learning_rate=0.0001,\n",
    "    weight_decay=1e-9,\n",
    "    scheduler_step_size=64,\n",
    "    scheduler_gamma=0.8,\n",
    "    max_norm=3.0\n",
    ")\n",
    "icnn_train_history = icnn_train_history_phase1 + icnn_train_history_phase2\n",
    "icnn_test_history = icnn_test_history_phase1 + icnn_test_history_phase2\n",
    "\n",
    "# -----------------------------\n",
    "# Plotting\n",
    "# -----------------------------\n",
    "\n",
    "x_test = torch.linspace(-15, 15, 200).reshape(-1, 1)\n",
    "y_true = f(x_test)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_nn_pred = nn_model(x_test)\n",
    "    y_icnn_pred = icnn_model(x_test)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(x_test, y_true, label='True $f(x) = x^2 - \\\\alpha x^4$', color='black')\n",
    "plt.plot(x_test, y_nn_pred, label='Standard NN', linestyle='--')\n",
    "plt.plot(x_test, y_icnn_pred, label='ICNN', linestyle='-.')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Test model\n",
    "plt.title(\"ICNN Train / Test Error\")\n",
    "plt.plot(np.sqrt(np.array(icnn_train_history))[10:], label=\"Train\")\n",
    "plt.plot(np.sqrt(np.array(icnn_test_history))[10:], label=\"Test\")\n",
    "plt.legend()\n",
    "#plt.yscale('log')\n",
    "plt.axis([0, None, 0, 10])\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"NN Train / Test Error\")\n",
    "plt.plot(np.sqrt(np.array(nn_train_history))[10:], label=\"Train\")\n",
    "plt.plot(np.sqrt(np.array(nn_test_history))[10:], label=\"Test\")\n",
    "plt.legend()\n",
    "#plt.yscale('log')\n",
    "plt.axis([0, None, 0, 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f0705c99-3e13-4003-93ef-4ff9c2c82d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case: case14 | Input Dimension: 3 | Num. Samples: 5000\n",
      "Number of valid samples:  4641\n",
      "Number of load buses:  11\n",
      "Loading models...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Training / Testing for IEEE Case Data\n",
    "# -----------------------------\n",
    "\n",
    "# cases = [\"case9\", \"case14\", \"case30\", \"case118\", \"case300\"]\n",
    "\n",
    "# Define case name, input dimension, number of samples\n",
    "case_name = \"case14\"\n",
    "input_dim = 3\n",
    "num_samples = 5000\n",
    "print(f\"Case: {case_name} | Input Dimension: {input_dim} | Num. Samples: {num_samples}\")\n",
    "\n",
    "curr_dir = pathlib.Path(\".\")\n",
    "data_dir = curr_dir / \"training_data\" / case_name\n",
    "model_dir = curr_dir / \"models\" / case_name\n",
    "model_dir.mkdir(exist_ok=True)\n",
    "\n",
    "### Load IEEE case data\n",
    "\n",
    "# Get case data\n",
    "case = cases[case_name].__dict__[case_name]()\n",
    "bus_data = case[\"bus\"]\n",
    "\n",
    "# Convert to dataframe\n",
    "bus_columns = [\"bus_ID\", \"bus_type\", \"Pd\", \"Qd\", \"Gs\", \"Bs\", \"area_number\", \"Vm\", \"Va\", \"basekV\", \"zone\", \"maxVm\", \"minVm\"]\n",
    "assert len(bus_columns) == bus_data.shape[1]\n",
    "\n",
    "# Organize data\n",
    "df_bus = pd.DataFrame(data=bus_data, columns=bus_columns)\n",
    "df_bus[[\"bus_ID\", \"bus_type\", \"area_number\", \"zone\"]] = df_bus[[\"bus_ID\", \"bus_type\", \"area_number\", \"zone\"]].astype(np.int64)\n",
    "df_bus = df_bus.set_index(\"bus_ID\")\n",
    "\n",
    "# Load buses\n",
    "load_buses = list(df_bus.index[(df_bus[[\"Pd\", \"Qd\"]] != 0.0).any(axis=1)])\n",
    "\n",
    "### Load IEEE case SDP OPF cost training data for ICNN\n",
    "\n",
    "# Load data\n",
    "df_data = pd.read_csv(data_dir / f\"training_{case_name}_{input_dim}d_{num_samples}samples.csv\")\n",
    "\n",
    "# Get feasible data points only\n",
    "df_data = df_data.loc[(df_data[\"Feas_Flag\"] == 1) & (df_data[\"Global_Opt\"] == 1)]\n",
    "print(\"Number of valid samples: \", len(df_data))\n",
    "\n",
    "# Organize data\n",
    "num_load_buses = int((len(df_data.columns) - 2) / 2)\n",
    "print(\"Number of load buses: \", num_load_buses)\n",
    "assert num_load_buses == len(load_buses)\n",
    "X_columns = [f\"Pd{i}\" for i in load_buses] + [f\"Qd{i}\" for i in load_buses]\n",
    "assert set(X_columns).issubset(set(df_data.columns))\n",
    "assert \"Cost\" in df_data.columns\n",
    "df_X = df_data[X_columns]\n",
    "df_y = df_data[\"Cost\"]\n",
    "\n",
    "# Get input data/labels\n",
    "X, y = df_X.values, df_y.values.reshape(-1, 1)\n",
    "\n",
    "### Train Neural Network(s)\n",
    "\n",
    "# Create training/testing data sets\n",
    "X_train, y_train, X_test, y_test = create_train_test_datasets(X, y)\n",
    "train_loader1 = create_batches(X_train, y_train, batch_size=200)\n",
    "train_loader2 = create_batches(X_train, y_train, batch_size=50)\n",
    "test_loader = create_batches(X_test, y_test, batch_size=100)\n",
    "\n",
    "# Initialize models\n",
    "icnn_model = ICNN(input_dim=2 * num_load_buses)\n",
    "nn_model = FCNN(input_dim=2 * num_load_buses)\n",
    "\n",
    "# Save paths for models\n",
    "nn_save_path = model_dir / f\"nn_{case_name}_{input_dim}d_{num_samples}samples.pth\"\n",
    "icnn_save_path = model_dir / f\"icnn_{case_name}_{input_dim}d_{num_samples}samples.pth\"\n",
    "\n",
    "train = False\n",
    "if train:\n",
    "    # Train models\n",
    "    \n",
    "    # Phase 1: SGD\n",
    "    nn_model, nn_train_history_phase1, nn_test_history_phase1 = train_model(\n",
    "        nn_model,\n",
    "        train_loader1,\n",
    "        test_loader,\n",
    "        optimizer=\"SGD\", \n",
    "        n_epochs=8000,\n",
    "        learning_rate=0.01,\n",
    "        weight_decay=1e-9,\n",
    "        scheduler_step_size=500,\n",
    "        scheduler_gamma=0.9,\n",
    "        max_norm=2.0\n",
    "    )\n",
    "    \n",
    "    # Phase 2: Adam\n",
    "    nn_model, nn_train_history_phase2, nn_test_history_phase2 = train_model(\n",
    "        nn_model,\n",
    "        train_loader2,\n",
    "        test_loader,\n",
    "        optimizer=\"Adam\", \n",
    "        n_epochs=8000,\n",
    "        learning_rate=0.0001,\n",
    "        weight_decay=1e-9,\n",
    "        scheduler_step_size=500,\n",
    "        scheduler_gamma=0.9,\n",
    "        max_norm=1.0\n",
    "    )\n",
    "    nn_train_history = nn_train_history_phase1 + nn_train_history_phase2\n",
    "    nn_test_history = nn_test_history_phase1 + nn_test_history_phase2\n",
    "    \n",
    "    # Phase 1: SGD\n",
    "    icnn_model, icnn_train_history_phase1, icnn_test_history_phase1 = train_model(\n",
    "        icnn_model,\n",
    "        train_loader1,\n",
    "        test_loader,\n",
    "        optimizer=\"SGD\",\n",
    "        n_epochs=8000,\n",
    "        learning_rate=0.1,\n",
    "        weight_decay=1e-9,\n",
    "        scheduler_step_size=500,\n",
    "        scheduler_gamma=0.8,\n",
    "        max_norm=3.0\n",
    "    )\n",
    "    \n",
    "    # Phase 2: Adam\n",
    "    icnn_model, icnn_train_history_phase2, icnn_test_history_phase2 = train_model(\n",
    "        icnn_model,\n",
    "        train_loader2,\n",
    "        test_loader,\n",
    "        optimizer=\"Adam\",\n",
    "        n_epochs=8000,\n",
    "        learning_rate=0.001,\n",
    "        weight_decay=1e-9,\n",
    "        scheduler_step_size=500,\n",
    "        scheduler_gamma=0.9,\n",
    "        max_norm=3.0\n",
    "    )\n",
    "    icnn_train_history = icnn_train_history_phase1 + icnn_train_history_phase2\n",
    "    icnn_test_history = icnn_test_history_phase1 + icnn_test_history_phase2\n",
    "\n",
    "else:\n",
    "    # Load models\n",
    "    print(\"Loading models...\")\n",
    "    load_model(nn_model, nn_save_path)\n",
    "    load_model(icnn_model, icnn_save_path)\n",
    "    print(\"Done.\")\n",
    "# Save models (optional)\n",
    "save = False\n",
    "if save:\n",
    "    model_dir = curr_dir / \"models\" / case_name\n",
    "    model_dir.mkdir(exist_ok=True)\n",
    "    save_model(nn_model, nn_save_path)\n",
    "    save_model(icnn_model, icnn_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c5a554-33b2-403a-ab19-9e74bf772bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show training/testing error\n",
    "plt.title(\"ICNN Train / Test Error\")\n",
    "plt.plot(np.sqrt(np.array(icnn_train_history))[10:], label=\"Train\")\n",
    "#plt.plot(np.sqrt(np.array(icnn_test_history))[10:], label=\"Test\")\n",
    "plt.legend()\n",
    "#plt.yscale('log')\n",
    "plt.axis([0, None, 0, 100])\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"NN Train / Test Error\")\n",
    "plt.plot(np.sqrt(np.array(nn_train_history))[10:], label=\"Train\")\n",
    "#plt.plot(np.sqrt(np.array(nn_test_history))[10:], label=\"Test\")\n",
    "plt.legend()\n",
    "#plt.yscale('log')\n",
    "plt.axis([0, None, 0, 100])\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average cost: $ {df_y.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c845bb89-12d8-47b0-8fd9-86a4759742a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting NN / ICNN predictions...\n",
      "Done.\n",
      "Getting local search OPF solver predictions...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "### Experiment ###\n",
    "# Compare NN, ICNN, and OPF solver on test data set\n",
    "\n",
    "# Create test data tensor\n",
    "test_loader = create_batches(X_test, y_test, batch_size=1, shuffle=False)\n",
    "\n",
    "# Get NN, ICNN predictions\n",
    "print(\"Getting NN / ICNN predictions...\")\n",
    "nn_pred_arr = pred(nn_model, test_loader)\n",
    "icnn_pred_arr = pred(icnn_model, test_loader)\n",
    "print(\"Done.\")\n",
    "\n",
    "# Get local search solver OPF predictions\n",
    "print(\"Getting local search OPF solver predictions...\")\n",
    "Pd_test, Qd_test = X_test[:, :num_load_buses], X_test[:, num_load_buses:]\n",
    "#lss_pred_arr = np.array([run_opf(case, Pd_test[i,:], Qd_test[i,:]) for i in range(X_test.shape[0])])\n",
    "print(\"Done.\")\n",
    "\n",
    "# Calculate residuals\n",
    "cost_arr = y_test.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d4f9f7fe-04a3-413d-a565-84ab3161a604",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_residuals = nn_pred_arr - cost_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f9a1d318-8320-4dc9-8a8b-76942696afd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "icnn_residuals = icnn_pred_arr - cost_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "07dcc8fc-5226-4960-a06b-e9c086260c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lss_residuals = lss_pred_arr - cost_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fe73639e-7d6b-44d5-94a4-babeecdeb4ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 35., 119., 192., 279., 278., 230., 139.,  78.,  36.,   7.]),\n",
       " array([  99.36135864,  839.46949489, 1579.57763115, 2319.68576741,\n",
       "        3059.79390366, 3799.90203992, 4540.01017617, 5280.11831243,\n",
       "        6020.22644869, 6760.33458494, 7500.4427212 ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(lss_pred_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16318275-b039-48a4-87ca-274c828070ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1137ee-1934-473e-8c7d-96069b1d28e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3437b37-0d3a-4441-8237-8704a6822add",
   "metadata": {},
   "source": [
    "### Next thing to try: generating data from... DC OPF?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370b7ad1-de84-40eb-b5ed-21aa9be0c0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandapower as pp\n",
    "import pandapower.networks as pn\n",
    "from pandapower import rundcopp\n",
    "from pandapower import OPFNotConverged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0709ae50-3618-4739-ba58-67ab885407d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_name = \"case9\"\n",
    "net = pn.case9()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583ed281-f3a2-42fa-9d82-536b4239e672",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_buses = net.load[\"bus\"]\n",
    "assert load_buses.is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4b08d8-217e-4982-93e0-252e2e6065e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 2 * len(load_buses) # Dimension of load vector subspace\n",
    "\n",
    "# Think of uniformly sampling a hyper-cube in d-dimensions... N = 50,000 times\n",
    "N = 5000\n",
    "\n",
    "# We can just generate points in the hyper-cube, and scale them by their original ranges of values... E.g. [0, max(p_mw)] x [0, max(q_mvar)]\n",
    "U = np.random.rand(N, d) # Uniform points in the hyper-cube\n",
    "Pd = U[:, 0:len(load_buses)] * net.load[\"p_mw\"].max()\n",
    "Qd = U[:, len(load_buses):] * net.load[\"q_mvar\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdcaf83-9617-4dcb-9caa-6d38ba4e8334",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=[f\"P{k}\" for k in load_buses] + [f\"Q{k}\" for k in load_buses] + [\"Cost\", \"Feas\"]\n",
    "Pd_columns = [f\"Pd{k}\" for k in load_buses]\n",
    "Qd_columns = [f\"Qd{k}\" for k in load_buses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec1280e-c132-4cea-8c7b-257d34d09705",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(columns = Pd_columns + Qd_columns + [\"Cost\", \"Feas\"])\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "for i in range(N):\n",
    "    net.load[\"p_mw\"] = Pd[i, :]\n",
    "    net.load[\"q_mvar\"] = Qd[i, :]\n",
    "    df_results.loc[i, Pd_columns] = net.load[\"p_mw\"].values\n",
    "    df_results.loc[i, Qd_columns] = net.load[\"q_mvar\"].values\n",
    "    try:\n",
    "        rundcopp(net)\n",
    "        feas = 1\n",
    "        cost = net.res_cost\n",
    "    except OPFNotConverged:\n",
    "        print(\"Infeasible.\")\n",
    "        feas = 0\n",
    "        cost = np.inf\n",
    "    df_results.loc[i, \"Cost\"] = cost\n",
    "    df_results.loc[i, \"Feas\"] = feas\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e73bd1a-b0a0-4f17-8f5e-124c560d1ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_csv(curr_dir / \"training_data\" / f\"DCOPF_{case_name}_{N}_samples.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64219770-e1c2-488e-acd5-b98830611305",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.read_csv(curr_dir / \"training_data\" / \"DCOPF_case9_5000_samples.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8f5587-2729-4a29-b6a9-61a0fbcd2b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feasible data points only\n",
    "df_data = df_results.loc[df_results[\"Feas\"] == 1]\n",
    "print(\"Number of samples: \", len(df_data))\n",
    "\n",
    "# Organize data\n",
    "num_buses = int((len(df_data.columns) - 2) / 2)\n",
    "print(\"Number of load buses: \", num_buses)\n",
    "X_columns = Pd_columns + Qd_columns\n",
    "assert set(X_columns).issubset(set(df_data.columns))\n",
    "assert \"Cost\" in df_data.columns\n",
    "df_X = df_data[X_columns]\n",
    "df_y = df_data[\"Cost\"]\n",
    "\n",
    "# Get input data/labels\n",
    "X, y = df_X.values.astype(np.float64), df_y.values.reshape(-1, 1).astype(np.float64)\n",
    "\n",
    "### Train Neural Network(s)\n",
    "\n",
    "# Initialize models\n",
    "icnn_model = ICNN(input_dim=2 * num_buses)\n",
    "nn_model = FCNN(input_dim=2 * num_buses)\n",
    "\n",
    "# Train models\n",
    "\n",
    "# Phase 1: SGD\n",
    "nn_model, nn_train_history_phase1, nn_test_history_phase1 = train_model(\n",
    "    nn_model,\n",
    "    X,\n",
    "    y,\n",
    "    batch_size=200,\n",
    "    optimizer=\"SGD\", \n",
    "    n_epochs=8000,\n",
    "    learning_rate=0.01,\n",
    "    weight_decay=1e-9,\n",
    "    scheduler_step_size=500,\n",
    "    scheduler_gamma=0.9,\n",
    "    max_norm=2.0\n",
    ")\n",
    "\n",
    "# Phase 2: Adam\n",
    "nn_model, nn_train_history_phase2, nn_test_history_phase2 = train_model(\n",
    "    nn_model,\n",
    "    X,\n",
    "    y,\n",
    "    batch_size=50,\n",
    "    optimizer=\"Adam\", \n",
    "    n_epochs=8000,\n",
    "    learning_rate=0.0001,\n",
    "    weight_decay=1e-9,\n",
    "    scheduler_step_size=500,\n",
    "    scheduler_gamma=0.9,\n",
    "    max_norm=1.0\n",
    ")\n",
    "nn_train_history = nn_train_history_phase1 + nn_train_history_phase2\n",
    "nn_test_history = nn_test_history_phase1 + nn_test_history_phase2\n",
    "\n",
    "# Phase 1: SGD\n",
    "icnn_model, icnn_train_history_phase1, icnn_test_history_phase1 = train_model(\n",
    "    icnn_model,\n",
    "    X,\n",
    "    y,\n",
    "    batch_size=200,\n",
    "    optimizer=\"SGD\",\n",
    "    n_epochs=8000,\n",
    "    learning_rate=0.1,\n",
    "    weight_decay=1e-9,\n",
    "    scheduler_step_size=500,\n",
    "    scheduler_gamma=0.8,\n",
    "    max_norm=3.0\n",
    ")\n",
    "\n",
    "# Phase 2: Adam\n",
    "icnn_model, icnn_train_history_phase2, icnn_test_history_phase2 = train_model(\n",
    "    icnn_model,\n",
    "    X,\n",
    "    y,\n",
    "    batch_size=50,\n",
    "    optimizer=\"Adam\",\n",
    "    n_epochs=8000,\n",
    "    learning_rate=0.001,\n",
    "    weight_decay=1e-9,\n",
    "    scheduler_step_size=500,\n",
    "    scheduler_gamma=0.9,\n",
    "    max_norm=3.0\n",
    ")\n",
    "icnn_train_history = icnn_train_history_phase1 + icnn_train_history_phase2\n",
    "icnn_test_history = icnn_test_history_phase1 + icnn_test_history_phase2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd2c9b4-c868-4259-94bc-7963caf63e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model\n",
    "plt.title(\"ICNN Train / Test Error\")\n",
    "plt.plot(np.sqrt(np.array(icnn_train_history))[10:], label=\"Train\")\n",
    "#plt.plot(np.sqrt(np.array(icnn_test_history))[10:], label=\"Test\")\n",
    "plt.legend()\n",
    "#plt.yscale('log')\n",
    "plt.axis([0, None, 0, 200])\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"NN Train / Test Error\")\n",
    "plt.plot(np.sqrt(np.array(nn_train_history))[10:], label=\"Train\")\n",
    "#plt.plot(np.sqrt(np.array(nn_test_history))[10:], label=\"Test\")\n",
    "plt.legend()\n",
    "#plt.yscale('log')\n",
    "plt.axis([0, None, 0, 200])\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average cost: $ {df_y.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1b5452-abc0-420b-82dd-201c3c34b5a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
